{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d9730e-4cfc-45ff-9a41-fbb0fdf10961",
   "metadata": {},
   "source": [
    "# Notebook batch-1831-land-cover-mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b182be2-614e-42cf-8a3f-57e66b27fec5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40238749-58c3-45f9-8e30-23793e71fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os\n",
    "import rasterio\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0df9e032-7eb8-44e0-8730-d9a24416e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Spécifiez le chemin vers votre fichier JSON téléchargé\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"data/gleaming-tube-438915-v7-9d3a41dc942d.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1dcba0-b3e6-4c61-a028-073437beea87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Approche globale \n",
    "\n",
    "- Consolider l'ensemble des bases de données pour le projet & répartir les images dans des dossiers train & val\n",
    "- Identifier, construire un modèle et l'entrainer le modèle en local puis avec TensorFlow DataSet à partir des buckets\n",
    "- Finetuner le modèle\n",
    "- Appliquer à google map (incl. API)\n",
    "- Construire l'interface\n",
    "- Investiguer les applications business/concrètes possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee74982-9ed2-4fa8-ad39-07c16c681fbf",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb39aa-b8d3-4fc4-abe2-1b2846ba4284",
   "metadata": {},
   "source": [
    "Télécharger l'ensemble des données, les consolder et construire le train, val, test set\n",
    "\n",
    "**Approche :**\n",
    "- Télécharger les set de données (OpenG & XBD), consolider l'ensemble\n",
    "- Identifier le nom des images pour les sets (train/val/test)\n",
    "- Processer les images (ex: np.transpose)\n",
    "- Calculer les différentes résolutions disponibles et les cas à appliquer (crop/diviser les images) pour faciliter l'entrainement\n",
    "- Vérifier si certaines données (esp. labels) sont manquantes\n",
    "- Construire les dossier train / val + test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d80595-372c-47d7-8810-5564fe12a44c",
   "metadata": {},
   "source": [
    "### Uploader les images manquantes de XBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c189af0-2a23-4c40-9f33-1bbc1e3cc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### IL VA FALLOIR UPLOADER LES ELEMENTS MANQUANTS ################\n",
    "\n",
    "# Code qui regarde un csv, cherche le bon fichier dans la BDD XBD, le transforme en numpy array (code Chat utile)\n",
    "# et le met dans le bon dossier avec le bon nom via la table de correspondance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496d406-a269-4403-885b-affa35f7fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅✅✅✅✅\n",
    "\n",
    "# On pourrait directement passer à l'étape construire les dossiers train/val/test mais je préfère avoir un bon raw data consolidée\n",
    "\n",
    "def xbd_paths(path):\n",
    "    #On charge le fichier de correspondance pour le manipuler et construire les chemins sources & destination\n",
    "    df = pd.read_csv(path, header = None, names = [\"source\", \"destination\"])\n",
    "    df['source'] = df['source'].str.replace(\".png\",  \".tif\")\n",
    "\n",
    "    #on construit les chemins sources où chercher les images\n",
    "    sources = list(df[\"source\"])\n",
    "    sources = [\"raw_data/XBD/images/\" + source for source in sources]\n",
    "\n",
    "    #on construit les chemins de destinations où iront les images\n",
    "    destinations = list(df[\"destination\"])\n",
    "    destination_paths = []\n",
    "    dossiers = []\n",
    "    for destination in destinations:\n",
    "        dossier = \"_\".join(destination.split(\"_\")[:-1])\n",
    "        dossiers.append(dossier) # Certains noms de ville sont composés, il faut tout prendre sauf ce qui vient après le dernier _\n",
    "        destination_paths.append(\"raw_data/\" + dossier + \"/images/\" + destination)\n",
    "    return sources, destination_paths, dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34c4565-8516-475a-8fba-ec53f4d2fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ✅✅✅✅✅ On construit les variables de paths pour copier les fichiers\n",
    "sources, destinations, dossiers = xbd_paths(\"raw_data/xbd_files.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb780f7-3c7b-479b-a1c3-a4d344059033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ✅✅✅✅✅ On prend chaque fichier, on le convertit en numpy array et on le met dans les bons dossiers\n",
    "\n",
    "for source, destination in zip(sources, destinations):\n",
    "    shutil.copy(source, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b397a0-7335-42db-a8c3-201be035b2b7",
   "metadata": {},
   "source": [
    "### Finaliser le listing des chemins pour construire les dossiers train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c5154-a066-4342-94cf-14fb7d7fa3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#✅✅✅✅✅ Création d'un listes avec les noms des fichiers à prendre\n",
    "\n",
    "#On construit une fonction qui va retourner le chemin vers 1)\n",
    "#les images du X 2) les images du y - en fonction du paramètre donnee qui est \"train\", \"test\", \"val\"\n",
    "def read_txt(donnee):\n",
    "    with open(f\"raw_data/{donnee}.txt\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        X = []\n",
    "        y = []\n",
    "        for line in lines:\n",
    "            region = line.split(\"_\")[0]\n",
    "            title = line.replace('\\n', '')\n",
    "            X.append(\"raw_data/\"+ region + \"/images/\" + title)\n",
    "            y.append(\"raw_data/\"+ region + \"/labels/\" + title)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fee1c20-c041-401e-b49f-db04e7ff94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅✅✅✅✅ On crée les listes pour les différents train et test\n",
    "X_train_set, y_train_set = read_txt(\"train\")\n",
    "X_val_set, y_val_set = read_txt(\"val\")\n",
    "X_test_set, y_test_set = read_txt(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea9a68-163b-44b1-b1f2-ce14013320c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ✅✅✅✅✅ On definit nos set de données finaux (quand X et y existent)\n",
    "X_train_set_final = []\n",
    "y_train_set_final = []\n",
    "train_files = []\n",
    "\n",
    "X_val_set_final = []\n",
    "y_val_set_final = []\n",
    "val_files = []\n",
    "\n",
    "X_test_set_final = []\n",
    "y_test_set_final = []\n",
    "test_files = []\n",
    "\n",
    "\n",
    "for X, y in zip(X_train_set, y_train_set):\n",
    "    if os.path.isfile(X) == True and os.path.isfile(y) == True:\n",
    "        X_train_set_final.append(X)\n",
    "        y_train_set_final.append(y)\n",
    "        train_files.append(X.split(\"/\")[-1])\n",
    "\n",
    "for X , y  in zip(X_test_set, y_test_set):\n",
    "    if os.path.isfile(X) == True and os.path.isfile(y) == True:\n",
    "        X_test_set_final.append(X)\n",
    "        y_test_set_final.append(y)\n",
    "        test_files.append(X.split(\"/\")[-1])\n",
    "\n",
    "\n",
    "for X, y  in zip(X_val_set, y_val_set):\n",
    "    if os.path.isfile(X) == True and os.path.isfile(y) == True:\n",
    "        X_val_set_final.append(X)\n",
    "        y_val_set_final.append(y)\n",
    "        val_files.append(X.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074b742-1183-43e2-9f39-f0d62fef439b",
   "metadata": {},
   "source": [
    "### Créer et peupler les dossiers train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298aec2c-2465-4aaf-807d-3cec8cc05d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ✅✅✅✅✅ On construit une fonction qui va aller chercher les images dont le chemin est\n",
    "#indiqué dans le set et les sauvegarder dans un dossier train ou val (en fonction du X et du y)\n",
    "i = 0\n",
    "# on copie les fichiers pour le train\n",
    "for X_train, y_train in zip(X_train_set_final, y_train_set_final):\n",
    "    dst_X_train = \"clean_data/train/images/\"\n",
    "    os.makedirs(os.path.dirname(dst_X_train), exist_ok=True)\n",
    "    shutil.copy(X_train, dst_X_train + X_train.split(\"/\")[-1])\n",
    "\n",
    "    dst_y_train = \"clean_data/train/labels/\"\n",
    "    os.makedirs(os.path.dirname(dst_y_train), exist_ok=True)\n",
    "    shutil.copy(y_train, dst_y_train + y_train.split(\"/\")[-1])\n",
    "\n",
    "\n",
    "#on copie les fichiers sur le val\n",
    "\n",
    "for  X_val, y_val in zip( X_val_set_final, y_val_set_final):\n",
    "\n",
    "    dst_X_val = \"clean_data/val/images/\"\n",
    "    os.makedirs(os.path.dirname(dst_X_val), exist_ok=True)\n",
    "    shutil.copy(X_val, dst_X_val + X_val.split(\"/\")[-1])\n",
    "\n",
    "    dst_y_val = \"clean_data/val/labels/\"\n",
    "    os.makedirs(os.path.dirname(dst_y_val), exist_ok=True)\n",
    "    shutil.copy(y_val, dst_y_val + y_val.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7dd960d-fb0b-4e3a-89d1-f12b10cf6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ✅✅✅✅✅ Fonction pour slicer une image\n",
    "def slicing(image_array, nb_tuiles):\n",
    "    tuiles_arrays = []\n",
    "    for tuiles_verticales in range(nb_tuiles):\n",
    "        for tuiles_horizontales in range(nb_tuiles):\n",
    "            tuiles_arrays.append(image_array[tuiles_verticales * 256 : 256 * (tuiles_verticales + 1),\\\n",
    "                                 tuiles_horizontales * 256: 256 * (tuiles_horizontales + 1)])\n",
    "    return tuiles_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "382bdaf4-a39a-4160-922c-46ccc30bcc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(\"clean_data/train/images/zanzibar_53.tif\") as src:\n",
    "    image_array = src.read()  # (nb_bands, height, width)\n",
    "    image_array = np.transpose(image_array, (1, 2, 0))\n",
    "tuiles_arrays = slicing(image_array, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f399128-7b0c-433c-a7a5-acea9d0b5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tuile(tuiles_arrays, train_file, data_set, images_or_label ):\n",
    "    path = \"processed_data/\" + data_set + \"/\" + images_or_label + \"/\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    for tuile_id, tuile_array in enumerate(tuiles_arrays):\n",
    "        if tuile_array.shape[-1] == 1 :\n",
    "            tuile_array = np.squeeze(tuile_array)\n",
    "        tuile_array = tuile_array.astype(\"uint8\")\n",
    "        image = Image.fromarray(tuile_array)\n",
    "        image.save(path + \"/\"  + train_file.split(\".\")[0] + \"_\" + str(tuile_id + 1) + \".tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819a730-a9a8-4c5c-9615-a2c0e2b6a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_tuile(data_set):\n",
    "    if data_set == \"train\":\n",
    "        files = train_files\n",
    "    elif data_set == \"val\":\n",
    "        files = val_files\n",
    "    path = \"clean_data/\" + data_set + \"/\"\n",
    "    #On ouvre chaque image X puis chaque image y. Commençons par les X\n",
    "    ## Ouverture des X - théoriquement les images seront de même taille\n",
    "    for file in files:\n",
    "\n",
    "        with rasterio.open(path + \"images/\" + file) as src:\n",
    "            image_array = src.read()  # (nb_bands, height, width)\n",
    "            image_array = np.transpose(image_array, (1, 2, 0))\n",
    "\n",
    "        #ensuite on regarde en combien de tuiles on peut le faire\n",
    "        nb_tuiles = (image_array.shape[0] // 256) # Sur une longueur ou hauteur on peut fiter nb_tuiles + un reste de pixels qui sera perdu\n",
    "        tuiles_totales = nb_tuiles **2 # ça nous aidera pour le nommage des tuiles (ex = aachen_1_2 - tuile 2 de la photo aachen_1\n",
    "        #il faudra donc slicer en nb_tuiles à la verticale et à la horizontale\n",
    "\n",
    "        #On va slicer le X\n",
    "        tuiles_arrays = slicing(image_array, nb_tuiles)\n",
    "        #on va sauvegarder les tuiles dans le fichier processed_data/train/images/X\n",
    "        save_tuile(tuiles_arrays, file, data_set, \"images\")\n",
    "\n",
    "        with rasterio.open(path + \"labels/\" + file) as src:\n",
    "            image_array = src.read()  # (nb_bands, height, width)\n",
    "            image_array = np.transpose(image_array, (1, 2, 0))\n",
    "\n",
    "        #on slide le y de la même manière que le X avec la même convention de nommage (donc le même nom\n",
    "\n",
    "        tuiles_arrays = slicing(image_array, nb_tuiles)\n",
    "        #on va sauvegarder les tuiles dans le fichier processed_data/train/images/X\n",
    "        save_tuile(tuiles_arrays, file, data_set, \"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae17bd-6b0d-4863-a34b-46bbe176ce88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#On lance le crop sur toutes les données de train et de val\n",
    "crop_tuile(\"train\")\n",
    "crop_tuile(\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff50bd-0fee-4f0b-83aa-9aa6ec67e9c5",
   "metadata": {},
   "source": [
    "## Construire le modèle U-NET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7d80d-4f4d-4274-987b-cfa4d696d809",
   "metadata": {},
   "source": [
    "### Définir le modèle UNET et l'initialiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2aea25-1bf9-4d18-84cb-13316af733ce",
   "metadata": {},
   "source": [
    "### [INUTILE POUR LE TRAIN AT SCALE GRACE AUX TENSORFLOW DATASETS] \n",
    "### Construire les X_train et X_val, y_train et y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "789ea84e-03cd-4acd-8481-29bc68f71def",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "path_images = \"data/processed_data/train/images/\"\n",
    "for path in os.listdir(path_images)[0:2000]:\n",
    "    with rasterio.open(path_images + path) as src:\n",
    "        image_array = src.read()  # (nb_bands, height, width)\n",
    "        image_array = np.transpose(image_array, (1, 2, 0))\n",
    "        X_train.append(image_array/255)\n",
    "\n",
    "path_labels = \"data/processed_data/train/labels/\"\n",
    "for path in os.listdir(path_labels)[0:2000]:\n",
    "    with rasterio.open(path_labels + path) as src:\n",
    "        image_array = src.read()  # (nb_bands, height, width)\n",
    "        image_array = np.transpose(image_array, (1, 2, 0))\n",
    "        y_train.append(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4fc7928-a6a5-4c36-be9c-8d8b1c5d7ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a765484-3788-4bc8-b1ca-e3924580abfa",
   "metadata": {},
   "source": [
    "## Tensorflow Data Set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5e95e-2686-41e0-89cc-d84b5630eab4",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999a98cf-3ea7-4a05-a4ed-69a30b59ecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images d'entraînement - Entrée : 35570, Cible : 35570\n",
      "Nombre d'images de validation - Entrée : 5931, Cible : 5931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 18:55:57.090159: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-12-11 18:55:57.090209: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2024-12-11 18:55:57.090215: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2024-12-11 18:55:57.090268: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-11 18:55:57.090296: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Définir le ratio des données à utiliser\n",
    "dataset_size_ratio = 1  # Par exemple, utiliser 50% des données\n",
    "\n",
    "# Fonction pour charger et prétraiter les images\n",
    "def load_and_preprocess_image(input_path, target_path, img_size=(256, 256)):\n",
    "    # Charger l'image d'entrée (couleur)\n",
    "    input_img = load_img(input_path, target_size=img_size)\n",
    "    input_array = img_to_array(input_img) / 255.0  # Normalisation entre 0 et 1\n",
    "\n",
    "    # Charger l'image cible (niveaux de gris)\n",
    "    target_img = load_img(target_path, target_size=img_size, color_mode='grayscale')\n",
    "    target_array = img_to_array(target_img)  # Normalisation entre 0 et 1\n",
    "\n",
    "    # Assurer que target_array a la forme (256, 256, 1)\n",
    "    target_array = target_array.reshape((256, 256, 1))  # Ajouter la dimension de canal\n",
    "\n",
    "    return input_array, target_array\n",
    "\n",
    "# Fonction pour obtenir les chemins des images\n",
    "def get_image_paths(input_dir, target_dir, img_format=\"*.tif\"):\n",
    "    input_paths = sorted(tf.io.gfile.glob(os.path.join(input_dir, img_format)))\n",
    "    target_paths = sorted(tf.io.gfile.glob(os.path.join(target_dir, img_format)))\n",
    "\n",
    "    # Vérification des chemins\n",
    "    for path in input_paths + target_paths:\n",
    "        if not tf.io.gfile.exists(path):\n",
    "            print(f\"Chemin introuvable : {path}\")\n",
    "\n",
    "    return input_paths, target_paths\n",
    "\n",
    "# Charger les chemins des images pour le jeu d'entraînement\n",
    "train_input_images_dir = \"data/processed_data/train/images\"\n",
    "train_target_images_dir = \"data/processed_data/train/labels\"\n",
    "train_input_paths, train_target_paths = get_image_paths(train_input_images_dir, train_target_images_dir)\n",
    "\n",
    "# Charger les chemins des images pour le jeu de validation\n",
    "val_input_images_dir = \"data/processed_data/val/images\"\n",
    "val_target_images_dir = \"data/processed_data/val/labels\"\n",
    "val_input_paths, val_target_paths = get_image_paths(val_input_images_dir, val_target_images_dir)\n",
    "\n",
    "# Limiter le nombre d'images en fonction du ratio défini\n",
    "train_limit = int(len(train_input_paths) * dataset_size_ratio)\n",
    "val_limit = int(len(val_input_paths) * dataset_size_ratio)\n",
    "\n",
    "train_input_paths = train_input_paths[:train_limit]\n",
    "train_target_paths = train_target_paths[:train_limit]\n",
    "\n",
    "val_input_paths = val_input_paths[:val_limit]\n",
    "val_target_paths = val_target_paths[:val_limit]\n",
    "\n",
    "# Fonction génératrice pour créer les batches\n",
    "def data_generator(input_paths, target_paths):\n",
    "    for input_path, target_path in zip(input_paths, target_paths):\n",
    "        try:\n",
    "            input_array, target_array = load_and_preprocess_image(input_path, target_path)\n",
    "            yield input_array, target_array\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec les fichiers : {input_path}, {target_path} - {e}\")\n",
    "\n",
    "# Dataset d'entraînement avec TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(256, 256, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32),\n",
    "    ),\n",
    "    args=(train_input_paths, train_target_paths)\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Dataset de validation avec TensorFlow\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(256, 256, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32),\n",
    "    ),\n",
    "    args=(val_input_paths, val_target_paths)\n",
    ")\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Affichage du nombre d'images d'entrée et cible pour chaque dataset\n",
    "print(f\"Nombre d'images d'entraînement - Entrée : {len(train_input_paths)}, Cible : {len(train_target_paths)}\")\n",
    "print(f\"Nombre d'images de validation - Entrée : {len(val_input_paths)}, Cible : {len(val_target_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0abff50-43cf-4e7d-8dba-9908e5ae4871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 18:56:10.022795: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1112/Unknown - 2840s 3s/step - loss: 1.5846 - accuracy: 0.4178"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 19:43:29.873578: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4566409315519817756\n",
      "2024-12-11 19:43:29.873598: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15046707941278834781\n",
      "2024-12-11 19:43:29.873605: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9719521468098157215\n",
      "2024-12-11 19:43:29.873609: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14184854330358018969\n",
      "2024-12-11 19:43:29.873614: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16866869980482363973\n",
      "2024-12-11 19:43:29.873617: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17822109207099364895\n",
      "2024-12-11 19:43:29.873620: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7827795555726796367\n",
      "2024-12-11 19:43:29.873628: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8000526961847678024\n",
      "2024-12-11 19:43:29.873638: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 2202279294062341778\n",
      "2024-12-11 19:43:30.281046: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-12-11 19:45:38.475270: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14494005880321759055\n",
      "2024-12-11 19:45:38.475286: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8930095002997400089\n",
      "2024-12-11 19:45:38.475300: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16591508304063833893\n",
      "2024-12-11 19:45:38.475309: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13336678077260093990\n",
      "2024-12-11 19:45:38.475312: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6113060831880400046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112/1112 [==============================] - 2969s 3s/step - loss: 1.5846 - accuracy: 0.4178 - val_loss: 1.5780 - val_accuracy: 0.3876\n",
      "Epoch 2/100\n",
      "1112/1112 [==============================] - 3061s 3s/step - loss: 1.3188 - accuracy: 0.4920 - val_loss: 1.5785 - val_accuracy: 0.3924\n",
      "Epoch 3/100\n",
      "1112/1112 [==============================] - 3055s 3s/step - loss: 1.2475 - accuracy: 0.5217 - val_loss: 1.5009 - val_accuracy: 0.4161\n",
      "Epoch 4/100\n",
      "1112/1112 [==============================] - 3059s 3s/step - loss: 1.1967 - accuracy: 0.5412 - val_loss: 1.4960 - val_accuracy: 0.4177\n",
      "Epoch 5/100\n",
      "1112/1112 [==============================] - 3091s 3s/step - loss: 1.1633 - accuracy: 0.5548 - val_loss: 1.4396 - val_accuracy: 0.4469\n",
      "Epoch 6/100\n",
      "1112/1112 [==============================] - 3102s 3s/step - loss: 1.1344 - accuracy: 0.5671 - val_loss: 1.4490 - val_accuracy: 0.4455\n",
      "Epoch 7/100\n",
      "1112/1112 [==============================] - 3057s 3s/step - loss: 1.1123 - accuracy: 0.5749 - val_loss: 1.4180 - val_accuracy: 0.4607\n",
      "Epoch 8/100\n",
      "1112/1112 [==============================] - 3054s 3s/step - loss: 1.0921 - accuracy: 0.5834 - val_loss: 1.3849 - val_accuracy: 0.4697\n",
      "Epoch 9/100\n",
      "1112/1112 [==============================] - 3055s 3s/step - loss: 1.0783 - accuracy: 0.5888 - val_loss: 1.3487 - val_accuracy: 0.4889\n",
      "Epoch 10/100\n",
      "1112/1112 [==============================] - 3054s 3s/step - loss: 1.0607 - accuracy: 0.5959 - val_loss: 1.3642 - val_accuracy: 0.4811\n",
      "Epoch 11/100\n",
      "1112/1112 [==============================] - 3054s 3s/step - loss: 1.0399 - accuracy: 0.6034 - val_loss: 1.3018 - val_accuracy: 0.5033\n",
      "Epoch 12/100\n",
      "1112/1112 [==============================] - 3053s 3s/step - loss: 1.0253 - accuracy: 0.6097 - val_loss: 1.3077 - val_accuracy: 0.4966\n",
      "Epoch 13/100\n",
      "1112/1112 [==============================] - 3053s 3s/step - loss: 1.0099 - accuracy: 0.6153 - val_loss: 1.3416 - val_accuracy: 0.5006\n",
      "Epoch 14/100\n",
      "1112/1112 [==============================] - 3054s 3s/step - loss: 0.9994 - accuracy: 0.6192 - val_loss: 1.2215 - val_accuracy: 0.5327\n",
      "Epoch 15/100\n",
      "1112/1112 [==============================] - 3055s 3s/step - loss: 0.9834 - accuracy: 0.6258 - val_loss: 1.2324 - val_accuracy: 0.5312\n",
      "Epoch 16/100\n",
      "1112/1112 [==============================] - 3056s 3s/step - loss: 0.9710 - accuracy: 0.6310 - val_loss: 1.2476 - val_accuracy: 0.5297\n",
      "Epoch 17/100\n",
      " 721/1112 [==================>...........] - ETA: 36:09 - loss: 0.9489 - accuracy: 0.6397  "
     ]
    }
   ],
   "source": [
    "## Callbacks\n",
    "es = EarlyStopping(patience = 4, restore_best_weights = True)\n",
    "checkpoint_path = 'models/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.2f}.h5'\n",
    "\n",
    "cp = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True)\n",
    "def unet(input_shape, num_classes):\n",
    "    inputs = Input(input_shape)\n",
    "    # Encoder Block 1\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    # Encoder Block 2\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    # Encoder Block 3\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    # Encoder Block 4\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    # Bridge\n",
    "    bridge = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    bridge = Conv2D(1024, (3, 3), activation='relu', padding='same')(bridge)\n",
    "    # Decoder Block 1\n",
    "    up1 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(bridge)\n",
    "    concat1 = Concatenate()([up1, conv4])\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(concat1)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    # Decoder Block 2\n",
    "    up2 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
    "    concat2 = Concatenate()([up2, conv3])\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(concat2)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    # Decoder Block 3\n",
    "    up3 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
    "    concat3 = Concatenate()([up3, conv2])\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(concat3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    # Decoder Block 4\n",
    "    up4 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
    "    concat4 = Concatenate()([up4, conv1])\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(concat4)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    # Output Layer\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(conv8)\n",
    "    # Model\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model\n",
    "model = unet((256, 256, 3),9)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model = unet((256, 256, 3),9)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_dataset, validation_data = val_dataset, batch_size = 32, epochs = 100, callbacks = [es,cp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf0a2a-51ca-494b-bdfa-4fdeced110f0",
   "metadata": {},
   "source": [
    "VertexAI connecté au bucket et qui prend 16 par 16 \n",
    "Sauvegarder le modèle toutes les X epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48bcb5-e3c4-4192-9c46-d5af5d912134",
   "metadata": {},
   "source": [
    "Matrice de confusion voir les classes dominantes, minoritaires, répartitions, voir sur quelle classe il n'y arrive pas \n",
    "Répartition par ville par tif\n",
    "benchmarker les modèles, et prendre un modèle pre-trained "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6b85d-44e7-45e7-b131-b5c8efbe9fc8",
   "metadata": {},
   "source": [
    "### Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf64872-b82a-4598-9cb4-f8a2603f2615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images d'entraînement - Entrée : 177, Cible : 177\n",
      "Nombre d'images de validation - Entrée : 29, Cible : 29\n",
      "Nombre d'images d'entraînement - Entrée : 177, Cible : 177\n",
      "Nombre d'images de validation - Entrée : 29, Cible : 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 18:55:19.384991: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-12-11 18:55:19.385031: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2024-12-11 18:55:19.385041: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2024-12-11 18:55:19.385091: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-11 18:55:19.385124: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Définir le ratio des données à utiliser\n",
    "dataset_size_ratio = 0.005  # Par exemple, utiliser 100% des données\n",
    "\n",
    "BUCKET_NAME = \"land-cover-mapping-lewagon1831\"\n",
    "TRAIN_IMAGES_PATH = \"processed_data/train/images/\"\n",
    "TRAIN_LABELS_PATH = \"processed_data/train/labels/\"\n",
    "VAL_IMAGES_PATH = \"processed_data/val/images/\"\n",
    "VAL_LABELS_PATH = \"processed_data/val/labels/\"\n",
    "\n",
    "# Initialiser le client GCS\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "# Fonction pour obtenir les chemins des images dans GCS\n",
    "def get_gcs_image_paths(bucket, prefix, img_format=\".tif\"):\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    paths = [blob.name for blob in blobs if blob.name.endswith(img_format)]\n",
    "    return sorted(paths)\n",
    "\n",
    "# Obtenir les chemins des fichiers dans le bucket\n",
    "train_input_paths = get_gcs_image_paths(bucket, TRAIN_IMAGES_PATH)\n",
    "train_target_paths = get_gcs_image_paths(bucket, TRAIN_LABELS_PATH)\n",
    "val_input_paths = get_gcs_image_paths(bucket, VAL_IMAGES_PATH)\n",
    "val_target_paths = get_gcs_image_paths(bucket, VAL_LABELS_PATH)\n",
    "\n",
    "# Limiter le nombre d'images en fonction du ratio défini\n",
    "train_limit = int(len(train_input_paths) * dataset_size_ratio)\n",
    "val_limit = int(len(val_input_paths) * dataset_size_ratio)\n",
    "\n",
    "train_input_paths = train_input_paths[:train_limit]\n",
    "train_target_paths = train_target_paths[:train_limit]\n",
    "val_input_paths = val_input_paths[:val_limit]\n",
    "val_target_paths = val_target_paths[:val_limit]\n",
    "\n",
    "# Vérification des chemins après le filtre du ratio\n",
    "print(f\"Nombre d'images d'entraînement - Entrée : {len(train_input_paths)}, Cible : {len(train_target_paths)}\")\n",
    "print(f\"Nombre d'images de validation - Entrée : {len(val_input_paths)}, Cible : {len(val_target_paths)}\")\n",
    "\n",
    "# Fonction pour charger une image depuis GCS\n",
    "def load_gcs_image(bucket, blob_name):\n",
    "    try:\n",
    "        blob = bucket.blob(blob_name)\n",
    "        image_data = blob.download_as_bytes()\n",
    "\n",
    "        # Vérification de l'extension de l'image\n",
    "        assert blob.name.endswith(('.tif', '.png', '.jpg', '.jpeg')), f\"Format d'image incorrect: {blob.name}\"\n",
    "\n",
    "        if blob.name.endswith(\".tif\"):\n",
    "            # Lire et convertir le fichier .tif en utilisant Pillow\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            image = image.convert(\"RGB\")  # Convertir en RGB\n",
    "            image_array = np.array(image, dtype=np.float32)\n",
    "            print(f\"Image chargée depuis GCS: {blob.name} - Shape: {image_array.shape}\")\n",
    "            return tf.convert_to_tensor(image_array, dtype=tf.float32)\n",
    "        else:\n",
    "            # Décodage standard pour PNG/JPEG\n",
    "            return tf.image.decode_image(image_data, channels=3)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement de l'image {blob_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour charger et prétraiter les images\n",
    "def load_and_preprocess_image(input_blob, target_blob, bucket, img_size=(256, 256)):\n",
    "    try:\n",
    "        # Charger l'image d'entrée (couleur)\n",
    "        input_img = load_gcs_image(bucket, input_blob)\n",
    "        assert input_img is not None, f\"Erreur dans le chargement de l'image d'entrée {input_blob}\"\n",
    "        print(f\"Chargement de l'image d'entrée: {input_blob} - Shape avant redimensionnement: {input_img.shape}\")\n",
    "        input_img = tf.image.resize(input_img, img_size) / 255.0  # Normalisation entre 0 et 1\n",
    "        print(f\"Shape de l'image d'entrée après redimensionnement et normalisation: {input_img.shape}\")\n",
    "\n",
    "        # Charger l'image cible (niveaux de gris)\n",
    "        target_img = load_gcs_image(bucket, target_blob)\n",
    "        assert target_img is not None, f\"Erreur dans le chargement de l'image cible {target_blob}\"\n",
    "        print(f\"Chargement de l'image cible: {target_blob} - Shape avant redimensionnement: {target_img.shape}\")\n",
    "        target_img = tf.image.resize(target_img, img_size)\n",
    "        target_img = tf.image.rgb_to_grayscale(target_img)  # Conversion en niveaux de gris\n",
    "        print(f\"Shape de l'image cible après redimensionnement et conversion en niveaux de gris: {target_img.shape}\")\n",
    "\n",
    "        return input_img, target_img\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du prétraitement des images {input_blob}, {target_blob}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Fonction génératrice pour créer les batches\n",
    "def data_generator(input_paths, target_paths, bucket):\n",
    "    print(f\"Début de la génération des batches\")  # Vérifier si le générateur est appelé\n",
    "    for input_blob, target_blob in zip(input_paths, target_paths):\n",
    "        print(f\"Traitement des images - Entrée: {input_blob}, Cible: {target_blob}\")\n",
    "        try:\n",
    "            # Charger et prétraiter les images\n",
    "            input_array, target_array = load_and_preprocess_image(input_blob, target_blob, bucket)\n",
    "\n",
    "            if input_array is None or target_array is None:\n",
    "                print(f\"Erreur dans le traitement des images {input_blob}, {target_blob}\")\n",
    "                continue\n",
    "\n",
    "            # Vérification des formes des tenseurs\n",
    "            print(f\"Forme de l'image d'entrée: {input_array.shape}\")\n",
    "            print(f\"Forme de l'image cible: {target_array.shape}\")\n",
    "\n",
    "            # Affichage d'une image pour validation visuelle\n",
    "            if random.random() < 0.1:  # Afficher 10% des images\n",
    "                plt.imshow(input_array)\n",
    "                plt.title(f\"Entrée: {input_blob}\")\n",
    "                plt.show()\n",
    "\n",
    "            yield input_array, target_array\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec les fichiers : {input_blob}, {target_blob} - {e}\")\n",
    "\n",
    "# Optimisation du dataset avec TensorFlow\n",
    "def create_dataset(input_paths, target_paths, bucket, batch_size=32, prefetch_buffer=tf.data.AUTOTUNE):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(input_paths, target_paths, bucket),  # Utilisation d'une fonction lambda pour passer le bucket\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(256, 256, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Optimisation du dataset : shuffle, cache et prefetch\n",
    "    dataset = dataset.batch(batch_size)  # Batch\n",
    "    dataset = dataset.cache()  # Cache les données après le premier passage\n",
    "    dataset = dataset.prefetch(prefetch_buffer)  # Pré-fetch pour améliorer la performance\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Création du dataset d'entraînement\n",
    "train_dataset = create_dataset(train_input_paths, train_target_paths, bucket)\n",
    "\n",
    "# Création du dataset de validation\n",
    "val_dataset = create_dataset(val_input_paths, val_target_paths, bucket, batch_size=32)\n",
    "\n",
    "# Affichage du nombre d'images d'entrée et cible pour chaque dataset\n",
    "print(f\"Nombre d'images d'entraînement - Entrée : {len(train_input_paths)}, Cible : {len(train_target_paths)}\")\n",
    "print(f\"Nombre d'images de validation - Entrée : {len(val_input_paths)}, Cible : {len(val_target_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb519962-34fd-4544-8874-325879b35f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 18:55:32.788910: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de la génération des batches\n",
      "Traitement des images - Entrée: processed_data/train/images/aachen_10_1.tif, Cible: processed_data/train/labels/aachen_10_1.tif\n",
      "Image chargée depuis GCS: processed_data/train/images/aachen_10_1.tif - Shape: (256, 256, 3)\n",
      "Chargement de l'image d'entrée: processed_data/train/images/aachen_10_1.tif - Shape avant redimensionnement: (256, 256, 3)\n",
      "Shape de l'image d'entrée après redimensionnement et normalisation: (256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "## Callbacks\n",
    "es = EarlyStopping(patience = 4, restore_best_weights = True)\n",
    "checkpoint_path = 'models/model_epoch_{epoch:02d}_val_accuracy_{val_accuracy:.2f}.h5'\n",
    "\n",
    "cp = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True)\n",
    "def unet(input_shape, num_classes):\n",
    "    inputs = Input(input_shape)\n",
    "    # Encoder Block 1\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    # Encoder Block 2\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    # Encoder Block 3\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    # Encoder Block 4\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    # Bridge\n",
    "    bridge = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    bridge = Conv2D(1024, (3, 3), activation='relu', padding='same')(bridge)\n",
    "    # Decoder Block 1\n",
    "    up1 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(bridge)\n",
    "    concat1 = Concatenate()([up1, conv4])\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(concat1)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    # Decoder Block 2\n",
    "    up2 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
    "    concat2 = Concatenate()([up2, conv3])\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(concat2)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    # Decoder Block 3\n",
    "    up3 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
    "    concat3 = Concatenate()([up3, conv2])\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(concat3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    # Decoder Block 4\n",
    "    up4 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
    "    concat4 = Concatenate()([up4, conv1])\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(concat4)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    # Output Layer\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(conv8)\n",
    "    # Model\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model\n",
    "model = unet((256, 256, 3),9)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model = unet((256, 256, 3),9)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_dataset, validation_data = val_dataset, batch_size = 16, epochs = 32, callbacks = [es,cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8bd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
